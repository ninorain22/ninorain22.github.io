## 1.4 Linux内核和传统Unix内核的比较

+ Linux支持动态加载内核模块
+ Linux支持对称多处理（SMP）机制
+ Linux内核可以抢占。Linux内核允许在内核运行的任务优先执行
+ Linux内核并不区分线程和一般进程
+ Linux提供具有设备类的面向对象的设备模型、热插拔时间，以及用户空间的设备文件系统(sysfs)
+ Linux忽略了STEAMS等特性

### 2.4.6 同步和并发
内核容易产生竞争条件，因此内核许多特性都要求能够并发访问共享数据，特别是：
+ Linux是抢占多任务操作系统。内核的进程调度即兴对进程进行调度和重新调度。内核必须和这些任务同步
+ Linux内核支持对称多处理系统（SMP），因此在两个以上处理器上执行的内核代码很可能会同时访问一个共享资源
+ 中断处理程序有可能访问同一资源
+ Linux内核可以抢占

常见解决竞争的办法是**自旋锁**和**信号量**


# 进程管理

## 3.1 进程
包括可执行程序代码、打开的文件、挂起的信号、内核内部数据、处理器状态、一个或多个具有内存映射的内存地址空间及一个或多个执行线程、用来存放全局变量的数据段

线程，是在进程中活动的对象。每个线程都拥有一个独立的程序计数器、进程栈河一组进程寄存器。内核调度的对象是**线程**，而不是进程。

进程提供2种虚拟机制：虚拟处理器和虚拟内存

在Linux中，进程通常是fork()的结果，该系统调用复制一个现有进程来创建另一个进程。在调用fork()结束时，再返回点这个相同位置上，父进程恢复执行，子进程开始执行。fork()系统调用从内核返回**2次**：1次返回到父进程，另一次返回到新产生的子进程

最终，程序通过exit()退出执行。这个函数会终结进程并将其占用的资源释放掉。

## 3.2 进程描述符及任务结构
内核把进程的列表存放在任务队列的**双向循环链表（task list)**中。链表的每一项都是类型为``task_struct``的进程描述符结构
task_struct在32位机器上大约1.7K。包含了：打开的文件、进程的地址空间、挂起的信号、进程的状态等

### 3.2.1 分配进程描述符
Linux通过**slab**分配器分配``task_struct``结构。在栈底或栈顶创建一个struct thread_info结构

struct thread_info {
    struct task_struct        *task;            // 指向该任务实际tast_struct的指针
    struct exec_domain        *exec_domain;
    __u32                     flags;
    __u32                     status;
    __u32                     cpu;
    int                       preempt_count;
    mm_segment_t              addr_limit;
    struct restart_block      restart_block;
    void                      *sysenter_return;
    int                       uaccess_err;
}
每个任务的thread_info在它的内核栈的尾端分配

### 3.2.2 进程描述符的存放
内核通过一个唯一的进程标识值或PID来标识每个进程。PID是一个数，表示为``pid_t``隐含类型，实际上是一个``int``类型
PID最大默认32768（short int最大），即系统中允许同时存在的进程的最大数目。

由于在内核中，访问任务一般需要获得指向其task_struct的指针。因此通过current宏找到正在运行进程的进程描述苻的速度就比较重要。
+ 有的硬件体系结构专门拿出一个寄存器来存放指向当前进程的tast_struct
+ 在X86这种体系结构（寄存器不富余），就只能在内核栈尾端创建thread_info结构，通过计算偏移间接的查找task_struct结构

### 3.2.3 进程状态
系统中每个进程必然处于下面5种状态之一：
+ TASK_RUNNING(运行)：进程是可执行的，或者正在执行，或在运行队列种等待执行。这是进程在用户空间中执行的唯一可能的状态；这种状态也可以应用到内核空间中正在执行的进程
+ TASK_INTERRUPTIBLE(可中断)：进程正在睡眠(被阻塞), 等待某些条件的达成。一旦条件达成，内核就会把进程状态设置为运行。处于此状态的进程页会因为接收到信号而被提前唤醒并准备投入运行
+ TASK_UNINTERRUPTIBLE（不可中断)：与可中断相同，就算是接收到信号也不会被唤醒或者准备投入运行。这个状态通常在进程必须等待时不受干扰或等待事件很快就会发生时出现，使用较少
+ __TASK_TRACED: 被其他进程跟踪的进程
+ __TASK_STOPPED(停止)：进程停止执行；没有投入运行也不能投入运行。

### 3.2.4 设置当前进程状态
set_task_state(task, state);    // 将任务task设置为state

### 3.2.5 进程上下文
当一个程序调用了系统调用或者触发某个异常，它就陷入了内核空间。

### 3.2.6 进程家族树
所有进程都是PID为1的init进程的后代。内核在系统启动的最后阶段启动init进程。该进程读取系统的初始化脚本并执行其他相关程序
进程间的关系存放在进程描述苻中，每个tast_struct都包含一个指向父进程task_struct的parent指针，还包含一个子进程链表children。

## 3.3 进程创建
Unix进程创建：
+ ``fork()``: 拷贝当前进程创建一个子进程，区别仅在于PID，PPID和某些资源和统计量（比如挂起的信号）
+ ``exec()``: 负责读取可执行文件并将其载入地址空间开始运行

### 3.3.1 写时拷贝
传统``fork()``系统调用直接把所有资源复制给新创建的进程。过于简单，效率低下。
Linux的``fork()``使用**写时拷贝**页实现：内核并不复制整个进程地址空间，只有在需要写入时，数据才会被复制，从而使各个进程拥有自己的拷贝。一般情况下，进程创建后都会马上运行一个可执行的文件。

### 3.3.2 fork()
Linux通过``clone()``系统调用实现``fork()``, ``fork()``、``vfork()``和``__clone()``都根据需要的参数标志去调用``clone()``,然后由``clone()``去调用``do_fork()``
``do_fork``调用``copy_process()``函数，然后让进程开始运行，``copy_process()``函数流程如下：
1. 调用``dup_task_struct()``为新进程创建一个内核栈、thread_info结构和task_struct，这些值与当前进程的值相同
2. 检查并确保新创建这个子进程后，当前用户所有的进程数没有超出给它分配的资源限制
3. 进程描述符内的非继承而来的成员都要被清0或者初始化。``task_struct``的大多数数据依然未被修改
4. 子进程状态被设置为TASK_UNINTERRUPTIBLE,保证它不会投入运行
5. ``copy_process()``调用``copy_flags()``以更新``task_struct``的``flags``成员。
6. 调用``alloc_pid()``为进程分配一个有效的PID
7. 根据传递给``clone()``的参数标志，``copy_process()``拷贝或共享打开的文件、文件系统信息、信号处理函数、进程地址空间和命名空间等
8. 扫尾工作并返回一个指向子进程的指针

再回到do_fork()函数，如果copy_process()函数成功返回，新创建的子进程被唤醒并让其投入运行。内核有意选择子进程首先执行。因为一般子进程都会马上调用exec()函数，这样可以避免写时拷贝的额外开销。如果让父进程首先执行的话，有可能会开始向地址空间写入。

### 3.3.3 vfork()
除了不拷贝父进程的页表项外，vfork()和fork()功能相同。通过向clone()传递一个特殊标志：
1. 在调用copy_process()时，task_struct的vfork_done设置为NULL
2. 在执行do_fork()时，如果给定特别标志，则vfork_done()会指向一个特定地址
3. 子进程先开始执行后，父进程不上马上恢复执行，而是一直等待，知道子进程通过vfork_done()指针向其发送信号
4. 在调用mm_release时，该函数用于进程退出内存地址空间，并检查vfork_done是否为空，如果不为空，会向父进程发送信号
5. 回到do_fork()，父进程醒来并返回

如果一切执行顺利，子进程在新的地址空间运行，而父进程页恢复了在原地址空间运行

## 3.4 线程在Linux中的实现
线程被是为一个与其他进程共享某些资源的进程。每个线程都有唯一隶属自己的task_struct。所以在内核中，它与普通的进程类似

### 3.4.1 创建线程
和创建普通进程类似，只是在``clone()``传递参数指明需要共享的资源
```
    clone(CLONE_VM | CLINE_FS | CLONE_FILES | CLONE_SIGHAND, 0)
```
和``fork()``差不多，只是父子共享地址空间、文件资源系统、文件描述符和信号处理程序
一个普通的fork()的实现是``clone(SIGCHLD, 0);``
而vfork()的实现是``clone(CLONE_VFORK | CLONE_VM | SIGCHLD, 0);``

### 3.4.2 内核线程                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
一种独立运行在内核空间的标准进程。内核线程和普通线程的区别在于内核线程没有**独立的地址空间**，它们只在内核空间运行，从来不切换到用户空间去。可以被调度，也可以被抢占
从现有内核线程中创建一个新的内核线程的方法如下：
struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char namefmt[], …)

如果不调用wake_up_process()明确唤醒它,它不会主动运行
内核线程启动后就一直运行到调用do_exit()退出，或者内核其他部分调用kthread_stop()退出

## 3.5 进程终结
大部分要靠``do_exit()``来完成：
+ 将task_struct中的标志成员设置为PF_EXITING
+ 调用del_timer_sync()删除任一内核定时器，根据返回的结果，确保没有定时器在排队，也没有定时器处理程序在运行
+ 如果BSD的进程记账功能是开启的，do_exit()调用acct_update_integrals()来输出记账信息
+ 调用exit_mm()来释放占用的mm_struct
+ 调用sem__exit()。如果进程排队等候IPC信号，则离开队列
+ 调用exit_files()和exit_fs(),以分别递减文件描述符、文件系统数据的引用计数，如果某个引用计数降为0，此时可以释放
+ 接着把存放在task_struct的exit_code成员中的任务退出代码置为由exit()提供的退出代码
+ 调用exit_notify()向父进程发起信号，给子进程重新找养父，养父为线程组中的其他线程或者init进程，并把进程状态设置为EXIT_ZOMBIE
+ 调用schedule()切换到新的进程

**do_exit()永不返回**

### 3.5.1 删除进程描述符
在调用了do_exit()之后，尽管进程已经不能再运行，但系统保留了它的进程描述符。这样可以让系统有办法在子进程终结后仍能获取得它的信息
在父进程获得已终结的子进程的信息后，或者通知内核它不再关注那些信息之后，子进程的task_struct才会被释放
wait()这一族函数都是通过唯一的一个系统调用wait4()来实现。标准动作是挂起调用它的进程，直到其中一个子进程退出，此时函数会返回该子进程的PID
当最终需要释放进程描述苻的时候，会调用release_task()：
+ 调用__exit_signal()，该函数调用_unhash_process(),后者又调用detach_pid()从pidhash上删除进程，同时也要从任务列表中删除进程
+ __exit_signal()释放目前僵死进程的所有使用的剩余资源
+ 如果这个进程是进程组最后一个进程，并且领头进程已经死掉，那么release_task()就要通知僵死的领头进程的父进程
+ release_task()调用put_task_struct()释放进程内核栈和thread_info结构所占的页，并释放task_struct所占的slab高速缓存

### 3.5.2 孤儿进程造成的进退维谷
在do_exit()中会调用exit_notify(),该函数会调用forget_original_parent(),而后调用find_new_reaper()来执行寻找新的父进程


# 进程调度

## 4.1 多任务
多任务系统可以划分为两类：非抢占式多任务和抢占式多任务
进程在被抢占前，能够运行的时间都是预先设置好的，叫进程时间片。但是在Linux的进程调度程序中，并没有采取时间片这个概念

## 4.2 Linux的进程调度
2.6版本中采用反转楼梯最后期限调度算法（RSDL），在Linux 2.6中使用了完全公平调度算法(CFS)

## 4.3 策略
### 4.3.1 I／O消耗型和处理器消耗型的进程
调度策略在两个矛盾的目标寻找平衡
+ 进程响应迅速
+ 最大系统利用率

Linux更倾向于优先调度I／O消耗型进程

### 4.3.2 进程优先级
Linux采用了2种不同的优先级范围。第一种是nice值。-20 ～ +19，nice值越大优先级越低
第二种是实时优先级。其值可以配置 0 ～ 99，越高代表优先级越高；任何实时进程的优先级都高于普通的进程。

### 4.3.3 时间片
表明进程在被抢占前所能持续运行的时间
Linux的CFS调度器并没有直接分配时间片到进程，而是将处理器的使用比例划分给了进程，这个比例还会受进程nice值的影响。
Linux系统是抢占式的。在Linux的新的CFS调度器中，其抢占时机取决于新的可运行程序消耗了多少处理器使用比。如果消耗的使用比比当前进程小，则新进程立即投入运行，抢占当前进程，否则推迟

## 4.4 Linux调度算法
### 4.4.1 调度器类
允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。每个调度器有一个优先级。基础调度器会按照优先级顺序遍历调度类。拥有一个可执行进程的最高优先级的调度器类胜出，去选择下面要执行的一个程序
完全公平调度（CFS）是一个针对普通进程的调度类

### 4.4.2 Unix系统中的进程调度
传统的Unix系统调度过程：
在Unix系统上，优先级以nice值形式输出给用户空间。
+ 问题1: 若将nice值映射到时间片，那必然要将nice单位对应到处理器的绝对时间。但这样做将导致进程切换无法最优化进行。
+ 问题2: 把nice值减小1所带来的效果极大地取决于其nice的初始值
+ 问题3: 最小时间片必然是定时器节拍的整数倍，也就是10ms或者1ms的倍数

CFS采用的方法是分配给进程一个处理器使用比重。确保了调度程序中有恒定的公平性，而将切换频率置于不断变动中

### 4.4.3 公平调度
CFS允许每个进程运行一段时间、循环轮转、选择运行最少的进程作为下一个运行进程。CFS在所有可运行进程总数基础上计算出一个进程应该运行多久，而不依靠nice值来计算。nice值在CFS中被作为进程获得的处理器运行比的权重。
每个进程都按照其权重在全部可运行进程中所占比例的*时间片*来运行。

任何进程所获得的处理器时间是由它自己和其他所有可运行进程的nice值的相对差值决定的。nice值对时间片的作用不再是算数加权，而是几何加权。任何nice值对应的时间是处理器的使用比。

## 4.5 Linux调度的实现
CFS的实现：
+ 时间记账
+ 进程选择
+ 调度器入口
+ 睡眠和唤醒

### 4.5.1 时间记账
所有的调度器都必须对进程运行时间记账。
#### 调度器实体结构
struct sched_entity {
    struct load_weight        load;
    struct rb_node            run_node;
    struct list_head          group_node;
    unsigned int              on_rq;
    u64                       exec_start;
    u64                       sum_exec_runtime;
    u64                       vruntime;
    u64                       prev_sum_exec_runtime;
    u64                       last_wakeup;
    u64                       avg_overlap;
    u64                       nr_migrations;
    u64                       start_runtime;
    u64                       avg_wakeup;
    ...
}

#### 虚拟实时
vruntime存放进程的虚拟运行时间(ns)，来记录一个程序到底运行了多长时间以及还应该再运行多久

### 4.5.2 进程选择
CFS会挑一个具有最小vruntime的进程。
CFS使用**红黑树**来组织可运行进程队列。红黑树称为rbtree,是一个自平衡二叉搜索树（在第6章里会详细介绍）
#### 挑选下一个任务
即运行rbtree最左叶子节点所带变的进程，而一般最左叶子节点会被缓存，这样更高效
#### 向树中加入进程
发生在进程被唤醒或者通过fork()调用第一次创建进程时。寻找合适的插入点，更新最左叶子节点缓存，以及一些统计数据等
#### 从树中删除进程
rbtree的rb_erase()函数+更新rb_leftmost缓存

### 4.5.3 调度器入口
主要入口点是函数schedule()，找到一个最高优先级的调度类，调度类由自己的可运行队列，调度类给出下一个该运行的进程。
从最高的优先级调度类开始，每一个调度类都实现了pick_next_task()函数，CFS中，pick_next_task()会调用pick_next_entity()

### 4.5.4 睡眠和唤醒
休眠的一个常见原因就是文件I／O，比如对一个文件执行了read()，而这操作需要从磁盘里读取。
内核针对该情况处理：进程把自己标记为休眠状态，从可执行rbtree中移除，放入**等待队列**。然后调用schedule()选择和执行一个其他进程。

休眠有2中相关的进程状态：TASK_INTERRUPTIBLE和TASK_UNINTERRUPTIBLE。唯一的区别就是出于TASK_UNINTERRUPTIBLE的进程会忽略信号，而处于TASK_INTERRUPTIBLE的进程可以被信号提前唤醒。

#### 等待队列
内核用wake_queue_head_t来代表等待队列。进程通过执行下面几步将自己加入到一个等待队列中：
1. 调用宏DEFINE_WAIT()创建一个等待队列的项
2. 调用add_wait_queue()把自己加入到队列中，该队列会在进程等待条件满足的时候唤醒它
3. 调用prepare_to_wait()将进程状态修改为TASK_INTERRUPTIBLE或者TASK_UNINTERRUPTIBLE
4. 如果状态被设置为TASK_INTERRUPTIBLE，则信号唤醒进程
5. 当进程被唤醒时，再次检查条件是否为真，如果是，就退出循环；如果不是，再次调用schedule()并一直重复这步操作
6. 当条件满足后，进程将自己设置为TASK_RUNNING，并调用finish_wait()将自己移出等待队列

#### 唤醒
通过wake_up()进行，会唤醒指定的等待队列上的**所有进程**。调用函数try_to_wake_up()，该函数负责将进程设置为TASK_RUNING状态，调用enqueue_task()将此进程放入rbtree中，如果被唤醒的进程优先级**高于**正在执行的进程，还要设置need_resched标志。

## 4.6 抢占和上下文切换
context_switch()函数负责处理可执行进程间的切换，完成2项基本工作
1. 调用switch_mm(),该函数负责把虚拟内存从上一个进程切换到新进程中
2. 调用switch_to()，该函数负责从上一个进程的处理器状态切换到新进程的处理器状态。包括保存、恢复栈信息和寄存器信息等

内核必须知道什么时候调用schedule()，如果仅靠用户代码显式调用schedule()，它们可能会永远执行下去。
内核提供了一个need_resched标志来表明是否需要重新执行一次调度。
当某个进程应该被抢占时，scheduler_tick()就会设置这个标志；当一个优先级高的进程进入可执行状态时，try_to_wake_up()也会设置这个标志，内核检查该标志，确认其被设置，调用schedule()来切换到一个新的进程。该标志传达给内核的意思就时：应该有其他进程被运行了，要尽快调用调度程序。
再返回用户空间以及从中断返回的时候，内核也会检查need_resched标志，如果被设置，内核会在继续执行之前调用调度程序

### 4.6.1 用户抢占
内核即将返回用户空间时，如果need_resched被设置，会导致schedule()被调用，此时就会发生**用户抢占**。
内核无论是从中断处理程序还是在系统调用后返回，都会检查need_resched标志，如果被设置了，就会调用schedule()选择一个合适的进程投入运行
总而言之，用户抢占发生在以下情况：
+ 从系统调用返回用户空间时
+ 从中断处理程序返回用户空间时

### 4.6.2 内核抢占
Linux完整支持内核抢占
只要没有持有锁，内核就可以进行抢占。**锁**是非抢占区域的标志。
为了支持内核抢占。第一处变动：为每个进程的thread_info引入preempt_count计数器，初始值为0，每次使用锁时+1，释放锁时-1。为0时，内核就可以抢占。
比如，从中断返回内核空间时，内核会检查need_resched和preempt_count的值，如果need_resched被设置，并且preempt_count为0的话，说明有一个更为重要的进程可以安全抢占。如果preempt_count不为0，此时抢占不安全，此时内核就会直接从中断返回当前执行进程。
如果内核中进程被阻塞了，或者显式的调用了schedule()，内核抢占也会显式的发生。内核抢占会发生在：
+ 中断处理程序正在执行，且返回内核空间之前
+ 内核代码再一次具有可抢占行时
+ 内核中任务显式地调用schedule()
+ 如果内核中的任务阻塞（也会导致调用schedule())

## 4.7 实时调度策略
普通的、非实时的调度策略是SCHED_NORMAL, Linux提供了2种实时调度策略：SCHED_FIFO和SCHED_RR

#### SCHED_FIFO
处于SCHED_FIFO级的进程优先级高于任何SCHED_NORMAL的进程。一个SCHED_FIFO级的进程会一直执行，直到它自己阻塞或者显式的释放处理器，不基于时间片。如果有多个同优先级的SCHED_FIFO级进程，会轮流执行。只有更高优先级的SCHED_FIFO或SCHED_RR任务才能抢占SCHED_FIFO任务。

#### SCHED_RR
与SCHED_FIFO类似，只是SCHED_RR级进程在耗尽事先分配的时间后就不能再继续执行。  

实时优先级范围从0～MAX_PT_PRIO（100）-1
而SCHED_NORMAL的nice值共享了这个取值空间：MAX_PT_PRIO~MAX_PT_PRIO+40

## 4.8 与调度相关的系统调用
### 4.8.1 与调度策略和优先级相关的系统调用
+ sched_setscheduler()：设置进程的调度策略
+ sched_getscheduler()：获取进程的调度策略
+ sched_setparam(): 设置进程的实时优先级
+ sched_getparam(): 获取进程的实时优先级
+ nice(): 将给定进程的静态优先级增加一个给定的量

### 4.8.2 与处理器绑定相关的系统调用
+ sched_setaffinity(): 设置进程task_struct的cpus_allowed位掩码标志。用于设置进程可以执行的cpu，即强制的处理器绑定
+ sched_getaffinity(): 返回当前的cpu_allowed位掩码

### 4.8.3 放弃处理器时间
Linux通过sched_yield()系统调用，显式地将处理器时间让给其他等待的进程。
通过将进程从活动队列中移到过期队列中，确保一段时间内不会再被执行。
而实时进程不会过期，它们只会被移到优先级队列的后面，不会放到过期队列中

# 系统调用
## 5.1 与内核通信
系统调用在用户空间进程和硬件设备之间加了一个中间层。该层主要有3个作用
+ 为用户空间提供了一种硬件的抽象接口
+ 保证系统的稳定和安全
+ 监管应用程序访问硬件，系统调用是用户空间访问内核的唯一手段

## 5.2 API、POSIX和C库
例子：
应用程序 调用printf() -> C库中的printf() -> C库中的write() -> 内核中的write()系统调用
Unix中，最流行的应用编程接口是基于POSIX标准的
Linux的系统调用像大多数Unix系统一样，作为**C库**的一部分提供。C库实现了Unix系统的主要API。

## 5.3 系统调用
### 5.3 1 系统调用号
在Linux,每个系统调用被赋予一个系统调用号。一旦分配就不能再改变。
内核记录了系统调用表中的所有已注册过的系统调用的表,sys_call_table

## 5.4 系统调用处理程序(system_call())
应用程序靠**软中断**通知内核机制，引发一个异常来促使系统切换到内核态去执行异常处理程序。此时的异常处理程序就是系统调用处理程序

### 5.4.1 指定恰当的系统调用
系统调用陷入内核时，必须把系统调用号一并传给内核。在X86上，系统调用号通过EAX寄存器传递给内核。

### 5.4.2 参数传递
除了系统调用号之外，还可能需要一些外部的参数输入。最简单的办法是将这些参数也放入寄存器中，在X86-32系统上, ebx, ecx, edx, esi和edi按照顺序存放前5个参数，如果超过5个参数，单独用一个寄存器存放所有指向这些参数在用户空间地址的指针

## 5.5 系统调用的实现
### 5.5.1 实现系统调用
时刻注意可移植性和健壮性

### 5.5.2 参数验证
最重要的一种检查就上检查用户提供的指针是否有效。内核提供了2个方法来完成必须的检查和内核空间与用户控制之间数据的来回拷贝
+ 为了向用户空间写数据，内核提供了copy_to_user()
+ 从用户空间读数据，copy_from_user()

copy_to_user()和copy_from_user()都有可能引起阻塞。

## 5.6 系统调用上下文
在进程上下文中，内核可以休眠并且可以被抢占。

# 内核数据结构
## 6.1 链表
### 单向链表和双向链表
struct list_element {
    void *data;
    struct list_element *next;
}

### 6.1.4 Linux内核中的实现
Linux不是将数据结构塞入链表，而是将链表节点塞入数据结构

#### 链表数据结构
struct list_head {
    struct list_head *next;
    struct list_head *prev;
}

struct fox {
    unsigned long tail_length;
    unsigned long weight;
    bool is_fantastic;
    struct list_head list;
}

// container_of可以方便的从链表指针找到父结构中包含的任何变量
#define container_of(ptr, type, member) ({
    const typeof(((type *)0)->member) * __mptr = (ptr);
    (type *)((char*)__mptr - offsetof(type, member));
})

// 使用container_of宏，可以定义一个简单的函数返回包含list_head的父结构体类型
#define list_entry(ptr, type, member)
    container_of(ptr, type, member)

#### 定义一个链表
struct fox *red_fox;
red_fox = kmalloc(sizeof(*red_fox), GFP_KERNAL);
red_fox->tail_length = 40;
red_fox->weight = 6;
red_fox->is_fantastic = false;
INIT_LIST_HEAD(&red_fox->list);

### 6.1.5 操作链表
#### 增加一个节点
list_add(struct list_head *new, struct list_head *head);

#### 删除一个节点
list_del(struct list_head *entry);
注意该操作并不会释放entry或包含entry的数据结构体所占用的内存，而是仅仅将entry从链表中移走。

#### 移动和合并链表节点
// 把节点从一个链表移动到另一个链表
list_move(struct list_head *list, struct list_head *head);

// 把节点从一个链表移到另一个链表末尾
list_move_tail(struct list_head *list, struct list_head *head);

// 判断是否为空
list_empty(struct list_head *head);

// 链接2个链表
list_splice(struct list_head *list, struct list_head *head);

### 6.1.6 遍历链表
#### 基本方法
struct list_head *p;
list_for_each(p, list) {
    // p指向链表中的元素
}

struct list_head *p;
struct fox *f;
list_for_each(p, &fox_list) {
    f = list_entry(p, struct fox, list);
}

#### 可用的方法
list_for_each_entry(pos, head, member);

## 6.2 队列
内核通用队列实现称为kfifo

### 6.2.2 创建队列
// 动态创建方法
int kfifo_alloc(struct kfifo *fifo, unsigned int size, gfp_t gfp_mask);

// 手动分配缓冲
void kfifo_init(struct kfifo *fifo, void *buffer, unsigned int size);

// 静态声明
DECLARE_KFIFO(name, size);
INIT_KFIFO(name);

size必须是2的幂

### 6.2.3 推入队列数据
unsigned int kfifo_in(struct kfifo *fifo, const void *from, unsigned int len);

### 6.2.4 取出队列数据
// 出队
unsigned int kfifo_out(struct kfifo *fifo, void *to, unsigned int len);

// 查看
// offset指向队列的索引位置，如果为0，则读队列头
unsigned int kfifo_out_peak(struct kfifo *fifo, void *to, unsigned int len, unsigned offset);

### 6.2.5 获取队列长度
// 获取用于存储kfifo队列的空间的总体大小
static inline unsigned int kfifo_size(struct kfifo *fifo);

// 获取kfifo队列中的已推入的数据大小
static inline unsigned int kfifo_len(struct kfifo *fifo);

// 获取还有多少可用空间
static inline unsigned int kfifo_avail(struct kfifo *fifo);

### 6.2.6 重制和撤销队列
static inline void kfifo_reset(struct kfifo *fifo);

// 撤销一个使用kfifo_alloc()分配的队列,用kfifo_free()
void kfifo_free(struct kfifo *fifo);

## 6.3 映射
也称关联数组
虽然散列是一种映射，但并非所有的映射都需要通过散列表实现。映射也可以通过自平衡二叉树存储数据。
Linux内核提供了一种非通用的映射：映射一个唯一的标识数（UID）

### 6.3.1 初始化一个idr
void idr_init(struct idr *idp);

### 6.3.2 分配一个新的UID
建立idr后，就可以分配UID了。分2步：
1. 告诉idr需要分配的新UID，允许其在必要时调整*后备树*大小
2. 请求新的UID

#### 调整后备树大小
int idr_pre_get(struct idr *idp, gfp_t gfp_mask);

#### 获取新的UID
// 成功获取一个新的UID，存在id中，而且将UID映射到ptr
int idr_get_new(struct idr *idp, void *ptr, int *id);

### 6.3.3 查找UID 
void *idr_find(struct idr *idp, int id);
### 6.3.4 删除UID
void idr_destroy(struct idr *idp);

## 6.4 二叉树
### 6.4.1 二叉搜索树
节点有序的二叉树，遵循以下规则：
+ 根的左分支节点都小于根节点值
+ 右分支节点值都大于根节点值
+ 所有的子树也都是二叉搜索树

### 6.4.2 子平衡的二叉搜索树
所有叶子节点的深度差不超过1的二叉搜索树

#### 红黑树
红黑树是一种自平衡搜索二叉树
+ 所有的节点要么是红色，要么是黑色
+ 叶子节点都是黑色
+ 叶子节点不包含数据
+ 所有非叶子节点都有2个子节点
+ 如果一个节点是红色，那么它的子节点都是黑色
+ 在一个节点到其他叶子节点的路径中，如果总是包含相同数目的黑色节点，则该路径相比其他路径是最短的

#### rbtree
Linux实现的红黑树称为rbtree

## 6.6 算法复杂度
略

# 中断和中断处理

## 7.1 中断
每个中断都通过一个唯一的数字标志，从而使得操作系统能够对中断区分。

### 异常
异常和中断不同，它在产生时必须考虑和处理器时钟同步。异常也称为同步中断。

## 7.2 中断处理程序
响应一个特定中断时，内核会执行中断处理程序。产生中断的每个设备都有一个响应的中断处理程序。
中断程序必须按照特定的类型声明，以便内核以标准的方式处理传递程序的信息。中断处理程序与其他内核函数的区别在于**中断处理程序是被内核调用来响应中断的**
他们运行在*中断上下文*中，也称为原子上下文，该上下问中的执行代码不可阻塞。

## 7.3 上半部与下半部的对比
一般把中断处理切分为两个部分，中断处理程序是上半部-接收到中断，立即开始执行，但只做有严格时限的工作；
能够被允许稍后完成的工作会推迟到下半部，此后在何时的时机，下半部会被开中断执行。
以网卡为例，当网卡接收来自网络的数据包时，需要通知内核数据包到来。网卡需要立即完成这件事，因此，网卡立即发出中断，通知硬件，拷贝最新的网络数据包到内存。然后读取网卡更多的数据包
当网络数据包被拷贝到系统内存后，中断任务就算完成，这时它将控制权交还给系统被中断前原先运行的程序
处理和操作数据包的其他工作在随后的下半部中进行

## 7.4 注册中断处理程序
每一设备都有相关的驱动程序，如果设备需要使用中断（大部分），那么相应的驱动程序就注册一个中断处理程序
驱动程序可以通过request_irq()注册一个中断处理程序，并激活给定的中断线
int request_irq(unsigned int irq,               // 要分配的中断号
                irq_handler_t handler,          // 指向处理这个中断的实际中断处理程序
                unsigned long flags,            // 中断处理程序标志，位掩码
                const char *name,               // 中断相关设备的ASCII文本表示
                void *dev)                      // 用于共享中断线，如果无需共享，传NULL即可

### 7.4.1 中断处理程序标志
+ IRQF_DISABLED: 内核在处理中断处理程序本身期间，禁止所有的其他中断
+ IRQF_SAMPLE_RANDOM: 表明这个设备产生的中断对*内核熵池*有贡献，如果设备以预知的速率产生中断，就不要设置该标志
+ IRQF_TIMER: 专门为系统定时器的中断准备
+ IRQF_SHARED: 表明可以在多个中断处理程序中共享中断线

request_irq()可能会睡眠，因此不能在中断上下文中调用该函数。在注册过程中，内核需要在/proc/irq文件中调用proc__mkdir()创建一个与中断对应的项。

### 7.4.3 释放中断处理程序
void free_irq(unsigned int irq, void *dev);

## 中断上下文
当执行一个中断处理程序时，内核处于中断上下文。
*进程上下文*是一种内核所处的操作模式，此时内核代表进程执行，在中断上下文中，可以通过current宏关联当前进程。
此外，因为进程是以进程上下文的形式连接到内核中的，因此，进程上下文可以睡眠，也可以调用调度程序

然而，中断上下文和进程没有什么关系。所以不可以睡眠，否则如何重新调度？
中断上下文具有较严格的时间限制，尽量把工作从中断处理程序中分离出来，放到下半部执行

中断处理程序栈的设置上一个配置选项。在2.6早期的版本中，中断处理程序拥有自己的栈，每个处理器一个，大小为1页，称为中断栈

## 7.7 中断处理机制的实现
设备产生中断，通过总线把电信号发送给中断控制器，如果中断线是激活的，那么中断控制器把中断发往处理器。处理器会立即停止它正在做的事，关闭中断系统，然后跳到内存中预定义的位置开始执行代码，这个预定义的位置上由内核设置的，即中断处理程序的入口

在内核中，中断开始于预定义入口点，对于每条中断线，处理器都会跳到对应的唯一的位置。这样，内核就知道所接收终端的IRQ号。然后，内核调用do_IRQ()。
计算出中断号后，do_IRQ()对所接收的中断进行应答，禁止这条线上的中断传递

# 下半部和推后执行的工作
中断处理程序只能完成整个中断处理流程的上半部分，是因为：
+ 中断处理程序以异步方式进行，并且有可能打断重要的代码，因此，中断处理程序应该执行的快
+ 如果当前有一个中断处理程序正在执行，在最好的情况下，该中断同级的其他中断都会被屏蔽，最坏的情况下，当前处理器上所有其他中断都会被屏蔽
+ 中断处理程序往往需要对硬件操作，所以时限要求较高
+ 中断处理程序不能在进程上下文中运行，所以不能阻塞

这样，整个中断处理流程就分为了两部分。第一部分是中断处理程序，第二部分就是下半部

## 8.1 下半部
下半部执行和处理密切相关但中断处理程序本身不执行的工作。划分上下半部工作的一些经验：
+ 如果一个任务对时间敏感，放在中断处理程序中执行
+ 如果和硬件相关，放到中断处理程序中执行
+ 如果一个任务保证不被其他中断打断，放到中断处理程序中执行
+ 其他所有任务，考虑放在下半部执行

### 8.1.1
通常下半部在中断处理程序一返回就会马上运行。下半部执行的时候，允许响应所有的中断

### 8.1.2 下半部的环境
在2.6版本中，内核提供了3中不同形式的下半部实现机制：软中断、tasklet和工作队列

必须保证在一个确定的时间段后再去运行下半部，就应该使用**内核定时器**

整理下，*下半部*是一个操作系统通用词汇，用于指代中断处理流程中推后执行的部分

## 8.2 软中断
### 8.2.1 软中断的实现
软中断是在编译期间静态分配的
struct softirq_action {
    void (*action)(struct softirq_action *);
}
static struct softirq_action softirq_vev[NR_SOFTIRQS];

每个被注册的软中断都占据数组的一项，因粗最多可以有32个软中断

#### 软中断处理程序
void softirq_handler(struct softirq_action *);

一个软中断不会抢占另一个软中断，实际上唯一可以抢占软中断的是中断处理程序

#### 执行软中断
通常，中断处理程序会在返回前标记它的软中断，使其在稍后被执行。于是，在合适的时刻，该软中断就会运行。软中断都要在do_softirq()中执行

### 8.2.2 使用软中断
软中断保留给系统中对时间要求最严格以及最重要的下半部使用，目前只有*网络*和*SCSI*直接使用软中断。此外，内核定时器和tasklet都是建立在软中断上的
#### 分配索引
在编译期间，定义一个枚举类型来静态的声明软中断。内核用这些从0开始的索引来表示一种相对优先级。索引号小的中断在大的之前执行
#### 注册处理程序
在运行时通过调用open_softirq()来注册软中断处理程序
open_softirq(NET_TX_SOFTIRQ, net_tx_action);

软中断处理程序执行的时候，允许响应中断，但自己不能休眠。在一个处理程序运行的时候，当前处理器上的软中断被禁止，但其他处理器可以执行**别的**软中断。

#### 触发中断
raise_softirq()可以将一个软中断设置为挂起状态，让它在下次调用do_softirq()函数时投入运行
例如，网络子系统调用raise_softirq(NET_TX_SOFTIRQ)，这会触发NET_TX_SOFTIRQ软中断，它的处理程序net_tx_action()就会在内核下一次执行软中断时投入运行。该函数在出发一个软中断之前，要**先禁止中断**，触发后再恢复原来的状态。
内核在执行完中断处理程序后，马上会调用do_softirq()函数，于是软中断开始执行中断处理程序留给它去完成的剩余任务

## 8.3 tasklet
tasklet是利用软中断实现的一种下半部机制
通常，应该优先使用tasklet，而不上软中断

### 8.3.1 tasklet的实现
#### tasklet结构体
struct tasklet_struct {
    struct tasklet_struct *next;        // 下一个tasklet
    unsigned long state;                // 状态
    atomic_t count;                     // 引用计数器
    void (*func)(unsigned long);        // tasklet处理函数
    unsigned long data;                 // 给tasklet处理函数的参数
}

其中，state只能是0、TASKLET_STATE_SCHED和TASKLET_STATE_RUN之间
count是tasklet引用计数，如果不为0，则tasklet被禁止，不允许执行，只有它为0时，tasklet才被激活，并且在被设置为挂起时，该tasklet才可以执行

#### 调度tasklet
已调度的tasklet(等同于被触发的软中断）存放在2个单处理器数据结构:tasklet_vec和tasklet_hi_vec。这2个都是由tasklet_struct结构体构成的链表
tasklet由tasklet_schedule()和tasklet_hi_schedule()调度
tasklet_schedule()执行步骤：
+ 检查tasklet状态是否为TASKLET_STATE_SCHED，如果是，说明被调度过了，立即返回
+ 调用_tasklet_schedule()
+ 保存中断状态，然后禁止本地中断
+ 把需要调度的tasklet加到每个处理器一个的tasklet_vec链表货tasklet_hi_vec链表的表头
+ 唤起TASKLET_SOFTIRQ或HI_SOFTIRQ软中断，这样，下一次调用do_softirq()时，就会执行该tasklet
+ 恢复中断到原状态并返回

大部分tasklet和软中断都是在中断处理程序中被设置为待处理状态，所以最近一个中断返回的时候就是执行do_softirq()的最佳时机，而这2歌处理程序,tasklet_action()和tasklet_hi_action()就是tasklet的处理核心

### 8.3.2 使用tasklet
大多数情况下，tasklet机制都是实现自己的下半部的最佳选择

#### 声明自己的tasklet
可以静态的创建tasklet，也可以动态的创建。静态的创建:
DECLARE_TASKLET(name, func, data);                // 引用计数器设置为0
DECLARE_TASKLET_DISABLED(name, func, data);       // 引用计数器设置为1

当该tasklet被调度后，给定的函数func会被执行，参数即data。上面2个宏的区别在于引用计数器的初始值不同。
还可以将一个间接引用（指针）赋给一个动态创建的tasklet_struct结构来初始化一个tasklet:
tasklet_init(t, tasklet_handler, dev);            // 动态创建

因为依靠软中断实现，所以tasklet不能睡眠。

## 8.4 工作队列
一般而言，如果推后执行的任务需要睡眠，就选择工作队列，否则，就选择软中断或者tasklet。
如果需要一个可以重新调度的实体来执行下半部处理，那么就应该使用工作队列

### 8.4.1 工作队列的实现
是一个用于创建内核线程的接口，通过它创建的进程负责执行由内核其他部分排到队列里的任务。这些内核线程称为**工作者线程**
工作队列子系统提供了一个缺省的工作者线程来处理这些需要推后的工作，缺省的工作者线程叫做events/n，n即处理器的编号。最好仅使用缺省线程

#### 工作者线程数据结构
struct workqueue_struct {
    struct cpu_workqueue_struct cpu_wq[NR_CPUS];        // 数组中的每一项对应系统中的一个处理器
    struct list_head list;
    const char *name;
    int singlethread;
    int freezeable;
    int rt;
}

struct cpu_workqueue_struct {
    spinlock_t lock;                        // 锁
    struct list_head worklist;              // 工作列表
    wait_queue_head_t more_work;
    struct work_struct *current_struct;
    struct workqueue_struct *wq;            // 关联工作队列结构
    task_t *thread;                         // 关联线程
}

每个处理器，每个工作者进程都对应一个cpu_workqueue_struct结构体
每个工作者进程类型关联一个自己的workqueue_struct，在该结构体内，给每个线程分配一个cpu_workqueue_struct，即给每个处理器分配一个。

#### 表示工作的数据结构
struct work_struct {
    atomic_long_t data;
    struct list_head entry;
    work_func_t func;
}

所有的工作者线程都是普通的内核线程实现的，他们都要执行worker_thread()函数，worker_thread()核心流程：
for (;;) {
    prepare_to_wait(&cwq->more_work, &wait, TASK_INTERRUPTIBLE);
    if (list_empty(&cwq->worklist))
        schedule();
    finish_wait(&cwq->more_work, &wait);
    run_workqueue(cwq);
}

## 8.5 下半部机制的选择
+ 如果被考察的代码本身多线索化的工作做的比较好，比如网络子系统，它完全使用单处理器变量，那么软中断就是非常好的选择
+ 如何代码的多线索化考虑的不够充分，那么选择tasklet，接口简单，而且两个同种类型的tasklet不能同时执行，所以实现简单。这也是驱动开发者应该尽可能选择的下半部机制
+ 如果需要把任务推后到进程上下文中完成，那么就只能选择工作队列

## 8.6 在下半部之间加锁
如果进程上下文和一个下半部共享数据，在访问这些数据之前，就需要禁止下半部的处理并得到锁的使用权
如果中断上下文和一个下半部共享数据，在访问数据之前，需要禁止中断并得到锁的使用权
任何在工作队列中被共享的数据也需要使用锁机制

## PS：第八章需要再次仔细揣摩，比较难理解

# 内核同步介绍

## 9.1 临界区和竞争条件
临界区就是访问和操作共享数据的代码段
如果两个执行线程有可能处于同一临界区中同时执行，那么就称作竞争条件。

## 9.2 加锁
Linux自身实现了几种不通锁机制，各种锁机制区别主要在于：当锁已经被其他线程持有，因而不可用时的行为表现——一些锁被争用时会简单的执行忙等待，而另外一些锁会使当前任务睡眠直到锁可用为止

### 9.2.1 造成并发执行的原因
用户空间之所以需要同步，是因为用户程序会被调度程序抢占和重新调度。
内核中有类似可能造成并发执行的原因：
+ 中断: 中断几乎可以在任意时刻异步发生，就可能随时打断当前正在执行的代码    
+ 软中断和tasklet: 内核能在任何时刻唤醒或者调度软中断和tasklet，打断正在执行的代码
+ 内核抢占: 内核可以抢占
+ 睡眠以及用户空间的同步: 在内核执行的进程可能会睡眠，这就会唤醒调度程序，从而调度一个新的用户进程执行
+ 对称多处理器: 两个或多个处理器可以同时执行代码

尽量在编写代码的开始阶段就要涉及恰当的锁

### 9.2.2 了解要保护什么
大多数内核数据结构都需要加锁。
如果有其他执行线程可以访问这些数据，那么就给这些数据加上某种形式的锁；
如果任何其他什么东西都能看到它，那么就要加锁；
要给数据加锁，而不是给代码加锁


## 9.3 死锁
死锁产生的条件：一个或多个执行线程和一个或多个资源，每个线程都在等待其中的一个资源，但是所有的资源都被占用来，且不会释放已占有的资源，于是任何线程都无法继续，这就是死锁
避免死锁的简单的规则：
+ 按序加锁：使用嵌套的锁时必须保证以相同的顺序获得锁
+ 防止发生饥饿
+ 不要重复请求同一个锁
+ 设计应尽量简单


# 10 内核同步方法

## 10.1 原子操作
内核提供了两组原子操作接口
+ 一组针对整数进行操作
+ 一组针对单独的位进行操作

### 10.1.1 原子整数操作
在不同的体系结构中实现原子操作时，使用atomic_t可以屏蔽其间的差异
typedef struct {
    volatile int counter;
} atomic_t;

使用atomic_t的代码只能将该类型的数据当作24位来用。是因为在SPARC体系结构中，原子操作的实现不同于其他体系结构：32位int类型的低8位被嵌入了一个锁。不过该问题已经解决。可以使用全部的32位

原子操作通常都是*内联函数*，往往是通过内嵌汇编指令来实现的
static inline int atomic_read(const atomic_t *v) 
{
    return v->counter;
}

### 10.1.2 64位原子操作
typedef struct {
    volatile long counter;
} atomic64_t;


## 10.2 自旋锁
Linux中最常见的锁就是*自旋锁*，自旋锁最多只能被一个可执行线程持有，如果一个执行线程试图获取一个已经被占有的自旋锁，该线程就会一直进行忙循环-旋转-等待锁重新可用。
一个被争用的自旋锁使得请求它的线程在等待锁重新可用时自旋（**非常浪费处理器时间**），所以自旋锁不应该被长时间持有。

### 10.2.1 自旋锁方法
基本使用形式如下：
DEFINE_SPINLOCK(mr_lock);
spin_lock(&mr_lock);
/* 临界区 */
spin_unlock(&mr_lock);

自旋锁在同一时刻最多被一个执行线程持有，所以同一时刻最多只有一个线程位于临界区
自旋锁可以使用在中断处理程序中。在获取锁之前，首先要禁止本地中断（当前处理器上的中断请求），否则，中断处理程序会打断正持有锁的内核代码，可能会试图争用这个已经被持有的自旋锁
 
禁止中断同时请求锁：
DEFINE_SPINLOCK(mr_lock);
unsigned long flags;
spin_lock_irqsave(&mr_lock, flags);
/* 临界区 */
spin_unlock_irqrestore($mr_lock, flags);
### 10.2.3 自旋锁和下半部
在与下半部配合使用时，必须小心使用锁机制。函数spin_lock_bh()用于获取制定锁，同时会禁止所有下半部的执行。spin_unlock_bh()执行相反的操作


## 10.3 读-写自旋锁
适用于某个数据结构的操作可以被划分为读／写或者消费者／生产者两种类别时
一个或多个读任务可用并发的豉油读者锁，用于写的锁只能被一个写任务持有
DEFINE_RWLOCK(mr_rwlock);
// 读者
read_lock(&mr_rwlock);
/* 临界区 */
read_unlock(&mr_rwlock);

// 写者
write_lock(&mr_rwlock);
/* 临界区 */
write_unlock(&mr_rwlock);

读-写自旋锁会更加偏向于读。大量的读者会使写者处于饥饿状态。
如果加锁时间不长且代码不会睡眠（例如中断处理程序），使用自旋锁是最佳选择，但是加锁时间长或者代码可能睡眠，那么最好用**信号量**来完成加锁

## 10.4 信号量
Linux中的信号量是一种睡眠锁，如果有一个任务试图获取一个已被占用的信号量时，信号量会将其推进一个等待队列，然后让其睡眠。处理器此时转去处理其他请求
根据锁被持有的时间长短来做判断选择信号量还是自旋锁。另外，信号量不同于自旋锁，它不会禁止内核抢占

### 10.4.1 计数信号量和二值信号量
信号量可用允许同时多个任务持有，在声明信号量时指定
制定为1的，就是互斥信号量，>1的就是计数信号量
在使用信号量时，基本用到的都是互斥信号量（计数=1的信号量）
down()操作对信号量计数-1来获取一个信号量，如果结果>=0，就获得信号量锁，否则任务会被放入等待队列，处理器执行其他任务
在临界区中的操作完成后，up()来释放信号量，来增加信号量的计数值
如果在该信号量上的等待队列不为空，那么处于队列中等待的任务在被唤醒的同时会获得该信号量

### 10.4.2 创建和初始化信号量
struct semaphore name;
sema_init(&name, count);

// 或者直接创建普通的互斥信号量
static DECLARE_MUTEX(name);

// 作为一个大数据结构的一部分动态创建
sema_init(sem, count);
init_MUTEX(sem);

### 10.4.2 使用信号量
down_interruptible()会在等待获取信号量时将进程设置为TASK_INTERRUPTIBLE状态，而down()会让进程进入TASK_UNINTERRUPTIBLE状态而不再响应信号
因此，使用down_interruptible()比down()更为正确

## 10.5 读-写信号量
所有读-写信号量都是互斥信号量（引用计数为1）
static DECLARE_RWSEM(name);
// 动态创建的初始化
init_rwsem(struct rw_semaphore *sem);

## 10.6 互斥体mutex
互斥体其实就是一种互斥信号
DEFINE_MUTEX(name);
// 动态
mutex_init(&mutex);

mutex_lock(&mutex);
/* 临界区 */
mutex_unlock(&mutex);

+ 任何时刻只有一个任务可以持有mutex,mutex计数永远是1
+ 给mutex上锁者必须负责再给其解锁，不能在一个上下文中锁定一个mutex，而在另一个上下文给其解锁
+ 不允许递归的上锁和解锁
+ 当持有一个mutex时，进程不可退出
+ mutex不能在中断或者下半部中使用
+ mutex只能通过官方API管理

### 10.6.1 信号量和互斥体
除非mutex的某个约束限制，否则相比信号量，优先使用mutex

### 10.6.2 自旋锁和互斥体
在中断上下文中只能使用自旋锁，而在任务睡眠时只能使用互斥体

## 10.7 完成变量
如果在内核中一个任务需要发出信号通知另一个任务发生了某个特定事件，利用*完成变量*是使两个任务得以同步的简单的方法。
DECLARE_COMPLETION(mr_comp);
// 动态    
init_completion(mr_comp);

####### 2017-08-28 18:24

## 10.8 BLK:大内核锁
全局自旋锁，为了方便从Linux最初的SMP过度到细粒度加锁机制

## 10.9 顺序锁
seq锁。2.6版本中引入的一种新型锁，用于读写共享数据。
依靠一个序列计数器，当有疑义的数据被写入时，会得到一个锁，并且序列值增加。在读取数据之前和之后，都会读取序号。如果读取的序号相同，那么说明在读操作过程中没有被写操作打断
此外，如果读取的值锁偶数，就表明没有写操作发生

// 定义一个seq锁
seqlock_t mr_seq_lock = DEFINE_SEQLOCK(mr_seq_lock);

// 写锁
write_seqlock(&mr_seq_lock);
/* 写锁被获取 */
write_sequnlock(&mr_seq_lock);

// 度
unsigned long seq;
do {
    seq = read_seqbegin(&mr_seq_lock);
    /* 读这里的数据 */
} while (read_seqretry(&mr_seq_lock, seq));

适用场景：
+ 数据存在很多读者
+ 写者很少
+ 写优先于读，而且不允许读者让写者饥饿
+ 数据结构简单

最有说服力的例子就是jiffies，jiffies是一个64位变量，记录了系统启动以来的时钟节拍累加数。获取它的方法是get_jiffies_64()
u64 get_giffies_64(void)
{
    unsigned long seq;
    u64 ret;

    do {
        seq = read_seqbegin(&xtime_lock);
        ret = jiffies_64;
    } while (read_seqretry(&xtime_lock, seq));
    return ret;
}

## 10.10 禁止抢占
略

## 10.11 顺序和屏障
确保顺序的执行称为屏障(barriers)

## 10.12 小结
最简单的确保同步的方法--原子操作
自旋锁--内核中最普通的锁，它提供了轻量级单独持有者的锁，即争用时忙等
信号量--一种睡眠锁
mutex--更通用的衍生锁


# 定时器和时间管理
内核需要相对时间和绝对时间
周期性时间的产生，都是由系统定时器驱动的

## 11.1 内核中的时间概念
系统定时器以某种频率自行触发时钟中断，该频率可以通过编程预定，称作**节拍率**

连续两次时钟中断的间隔时间，称为**节拍**

## 11.2 节拍率：HZ
X86体系结构中，系统定时器频率默认为100，因此X86时钟中断频率为100HZ，即每秒中断100次
大多数体系结构的节拍率是可以调的

### 11.2.1 理想的HZ值
Linux 2.5开发版内核的中断频率提高到1000HZ。
提高节拍率等同于提高中断解析度，如果HZ=100的时钟执行粒度为10ms, 即系统中周期事件最快为每10ms运行一次。当HZ=1000时，执行粒度就为1ms
另外，提高解析度的同时也提高了准确度，比如内核在某个随机时刻触发定时器，而它可能在任何时间超时，但由于只有在时钟中断到来才能执行，因粗平均误差为半个时钟

### 11.2.2 高HZ的优势
+ 内核定时器频度和精准度更高
+ 依赖定时值执行的系统调用, 比如poll()和select()，能以更高的精度运行
+ 对资源消耗和系统运行时间的测量更精细
+ 提高进程抢占的准确度

### 11.2.3 高HZ的劣势
+ 时钟中断频率高，系统负担月中，必须花更多时间来处理时钟中断程序
现代计算机上，1000HZ不会对系统性能造成较大的影响

## 11.3 jiffies
全局变量jiffies用来记录系统自启动以来产生的节拍的总数
内核给jiffies附一个特殊的初值，引起这个变量不断的溢出，由此捕捉bug。当找到实际的jiffies后，就首先把这个偏差减去

### 11.3.1 jiffies的内部表示
jiffies总是无符号长整数（unsigned long)

```
volatile关键字：volatile关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改
volatile关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如：操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。
volatile 指出 i是随时可能发生变化的，每次使用它的时候必须从i的地址中读取，因而编译器生成的汇编代码会重新从i的地址读取数据放在b中。而优化做法是，由于编译器发现两次从i读数据的代码之间的代码没有对i进行过操作，它会自动把上次读的数据放在b中。而不是重新从i里面读。这样以来，如果i是一个寄存器变量或者表示一个端口数据就容易出错，所以说volatile可以保证对特殊地址的稳定访问
```
### 11.3.2 jiffies的回绕
节拍计数增加到最大值2的31次方-1，它的值会回绕到0
内核提供了四个宏来帮助比较节拍计数，他们能正确的处理节拍计数回绕清空
time_after()    
time_before()
time_after_eq() 
time_before_eq()

## 11.4 硬时钟和定时器
体系结构提供了两种设备进行计时
1. 系统定时器
2. 实时时钟

### 11.4.1 实时时钟
用来持久存放系统时间的设备
系统启东市，内核读取RTC来初始化墙上时间，该时间存放在xtime变量中。

### 11.4.2 系统定时器
周期性触发中断机制（可能是电子晶振，可能是衰减测量器）
在x86体系结构中，主要采用可编程中断时钟（PIT）

## 11.5 时钟中断处理程序
划分为2部分：**体系结构相关部分**和**体系结构无关部分**

与体系结构相关的例程作为系统定时器的中断处理程序注册到内核中，完成以下操作：
+ 获取xtime_lock锁，对访问jiffies_64和墙上时间xtime进行保护
+ 需要时应答或重新设置时钟
+ 周期性地使用墙上时间更新时钟
+ 调用体系机构无关的时钟例程

## 11.7 定时器
内核定时器可以用来将部分代码推迟到下半部执行
```
struct timer_list {
    struct list_head entry;     // 定时器链表入口
    unsigned long expires;      // 以jiffies为单位的定时值
    void (*function) (unsigned long);    // 定时器处理函数
    unsigned long data;         // 传给处理函数的长整形参数
    struct tvec_t_base_s *base;     // 定时器内部值
}
```

使用
```
// 创建
struct timer_list my_timer;
// 初始化
init_timer(&my_timer);
// 填充值
my_timer.expires = jiffies + delay; // 超时节拍数
my_timer.data = 0;
my_timer.function = my_function;
// 激活
add_timer(&my_timer);
```

### 11.7.2 定时器竞争条件
一般情况下，应该使用del_time_sync()来取代del_timer()函数，因为无法确定在删除定时器时，它是否还正在其他处理器上运行

### 11.7.3 实现定时器
定时器作为软中断在下半部上下文中执行，具体来说，时钟中断处理程序会执行update_process_time()函数，该函数随机调用run_local_timers()
```
void run_local_timers(void)
{
    hrtimer_run_queues();
    raise_softirq(TIMER_SOFTIRQ); // 执行定时器软中断
    softlockup_tick();
}
```

## 11.8 延迟执行
内核代码，尤其是驱动程序，除了使用定时器或者下半部机制意外，还需要其他其他方法来推迟执行任务。比如重新设置网卡的以太模式需要2ms，所以驱动程序必须至少等待2ms才能继续运行

### 11.8.1 忙等待
仅适用于延迟的时间是节拍的整数倍，或者精确率要求不高
几乎不用

更好的方法是代码等待时，允许内核重新调度执行其他任务
```
unsigned long delay = jiffies + 5*HZ;
while (time_before(jiffies, delay))
    cond_resched(); // 将调度一个新程序投入运行
```
因为cond_resched()需要调用调度程序，因此它不能在中断上下文中使用——只能在进程上下文中使用
jiffies字段被标记为volatile，指示编译器每次访问变量时都重新从主内存中获得，而不是直接访问寄存器中的变量名

### 11.8.2 短延迟
有的代码要求延迟时间短，而且要求延迟时间很精确
内核提供了3个可以处理ms,us级别的延迟函数
```
void udelay(unsigned long usecs);
void ndelay(unsigned long nsecs);
void mdelay(unsigned long msecs);
```

BogoMIPS:
MIPS(每秒处理百万条指令)
BogoMIPS记录处理器在给定时间内忙循环执行的次数


### 11.8.3 schedule_timeout()
更理想的延迟执行方法是schedule_timeout()函数
该方法会让需要延迟执行的任务睡眠到指定的延迟时间后再重新运行
```
set_current_state(TASK_INTERRUPTIBLE);
schedule_timeout(s*HZ); // s秒后唤醒
```

由于schedule_timeout()需要调用调度程序，所以调用它的代码必须保证能够睡眠。因此调用代码必须处于进程上下文中，且不能持有锁
```
signed long schedule_timeout(signed long timeout)
{
    timer_t timer;
    unsigned long expire;
    
    switch (timeout) {
        case MAX_SCHEDULE_TIMEOUT: 
            schedule();
            goto out;
        default:
            if (timeout < 0) {
                printk(KERN_ERR "***");
                current->state = TASK_RUNNING;
                goto out;
            }
    }
    
    expire = timeout + jiffies;
    
    init_timer(&timer);
    timer.expires = expire;
    timer.data = (unsigned long) current;
    timer.function = process_timeout;
    
    add_timer(&timer);
    schedule();
    del_timer_sync(&timer);
    
    timeout = expire - jiffies;
    
    out:
        return timeout < 0 ? 0 : timeout;
}
```
定时器超时，process_timeout()会调用
```
void process_timeout(unsigned long data)
{
    wake_up_process((task_t *) data);
}
```
该函数将任务设置为TASK_RUNNING状态，然后放入运行队列


# 内存管理

## 12.1 页
内存管理单元（MMU）把物理页作为内存管理的基本单位，也是从虚拟内存角度来看的最小单位
内核用struct page结构表示系统中的每个物理页
```
struct page {
    unsigned long flags;        // 页的状态，是不是脏的，是不是被锁定在内存中，每一位单独表示一种状态
    atomic_t _count;            // 页的引用计数
    atomic_t _mapcount;
    unsigned long private;
    struct address_space *mapping;
    pgoff_t index;
    struct list_head lru;
    void *virtual;              // 页的虚拟地址，通常情况下，有些内存并不永久地映射到内核空间上，这时，该值为NULL
}
```

## 12.2 区
有些页位于内存中特定的物理地址上，所以不能用于一些特定的任务。所以内核把页划分为不同的区
内核对具有相似特性的页分组。Linux必须处理如下两种硬件存在缺陷而引起的内存熏制问题
+ 一些硬件只能用某些特性的内存地址来执行DMA（直接内存访问）
+ 一些体系结构的内存的物理殉职范围比虚拟寻址范围大得多，因此有些内存不能永久映射到内核空间上

Linux主要使用了4种区：
1. ZONE_DMA - 这个区包含的页只能用来执行DMA操作
2. ZONE_DMA32 - 和ZONE_DMA类似，只是这些页面只能被32位设备访问
3. ZONE_NORMAL - 能正常映射的页
4. ZONE_HIGHEM - 包含*高端内存*，其中的页并不能永久地映射到内核地址空间
在32位x86系统上，ZONE_HIGHEM为高于896MB的所有物理内存。在其他体系结构上，由于所有内存都被直接映射，所以ZONE_HIGHEM为空
每个区都用struct zone表示
```
struct zone {
    unsigned long               waternark[NR_WMARK];            // 持有该区的最小值、最低和最高水位值，该水位随着空闲内存的多少而变化
    unsigned long               lowmen_reserve[MAX_NR_ZONES];
    struct per_cpu_pageset      pageset[NR_CPUS];
    spinlock_t                  lock;                           // 自旋锁，防止该结构被并发访问
    struct free_area            free_area[MAX_ORDER];
    spinlock_t                  lru_lock;
    struct zone_lru {
        struct list_head list;
        unsigned long nr_saved_scan;
    } lru[NR_LRU_LISTS];
    struct zone_recliam_stat    reclaim_stat;
    unsigned long               pages_sacnned;
    unsigned long               flags;
    atoimic_long_t              vm_stat[NR_VM_ZONE_STAT_ITEMS];
    int                         prev_priority;
    unsigned int                inactive_ratio;
    wait_queue_head_t           *wait_table;
    unsigned long               wait_table_hash_nr_entries;
    unsigned long               wait_table_bits;
    struct pglist_data          *zone_pgdat;
    unsigned long               zone_start_pfn;
    unsigned long               spanned_pages;
    unsigned long               present_pages;
    const char                  *name;                        // 'DMA'|'Normal'|'HignMem' 
}
```

## 12.3 获得页
``struct page * alloc_pages(gfp_t gfp_mask, unsigned int order)``
该函数分配2的order次方个连续的物理页，并返回一个指向第一个页page结构体的指针
可以用这个函数把给定的页转换成它的逻辑地址：
``void * page_address(struct page *page)``
可以调用这个函数来获得页：
``unsigned long __get_free_pages(gfp_t gfp_mask, unsigned long order)``
它直接返回第一个页的逻辑地址

如果只需要一页, order参数可以不传

### 12.3.2 释放页
```
void __fres_pages(struct page * page, unsigned int order);
void free_pages(unsigned long addr, unsigned int order);
void free_pages(unsigned long addr);
```

## 12.4 kmalloc()
用于获得以字节为单位的一块内核内存``void * kmalloc(size_t size, gfp_t flags);``
返回一个指向内存块的指针

### 12.4.1 gfp_mask标志
分为3类：
1. 行为修饰符：表示内核应当如何分配所需内存，例如中断处理程序要求内核在分配内存过程中不能睡眠
2. 区修饰符：指明到底从这些区中的哪一区进行分配
3. 类型：组合了行为修饰符和区修饰符，例如GFP_KERNEL（也是首选标示）

GFP_KERNEL可能会引起睡眠，因为它使用的是普通优先级。
GFP_ATOMIC标示不能睡眠的内存分配，适用于中断处理程序、软中断和tasklet等不能睡眠的情况）

### 12.4.2 kfree()
``void kfree(const void *ptr);``
释放由kmalloc()分配出来的内存块


## 12.5 vmalloc()
类似于kmalloc()，只是vmalloc()分配的内存虚拟地址是连续的，而物理地址则无须连续。而kmalloc()确保返回的页在物理地址上都是连续的
相比于kmalloc()，vmalloc()为了把物理上不连续的页转换为虚拟地址连续的页，必须专门建立页表想。这回导致比直接内存映射大得多的TLB（硬缓冲区，缓存虚拟地址到物理地址的映射关系）抖动
``void * vmalloc(unsigned long size);``
释放：``void vfree(consty void *attr);``，这个函数可以睡眠，因此不能从中断上下文中调用

## 12.6 slab层
slab分配器就是一种通用数据结构缓存层

### 12.6.1 slab层的设计
slab层把不同的对象划分为高速缓存组，每个组都存放不同类型的对象，每种对象类型对应一个高速缓存
然后，这些高速缓存又被划分为slab。slab由一个或者多个物理上连续的页组成。一般情况，slab仅由一页组成
每个slab处于三种状态之一：满、部分满、空
当内核需要一个新对象时，优先从部分满的slab中进行分配

slab描述符：
```
struct slab {
    struct list_head list;      // 满、部分满或空链表
    unsigned long colouroff;    // slab着色的偏移量
    void $s_mem;                // 在slab中的第一个对象
    unsigned int inuse;         // slab中已经分配的对象数
    kmem_bufctl_t free;         // 第一个空闲对象
}
```
slab层的关键就是避免频繁分配和释放页：
只有在给定的高速缓存部分中既没有满、也没有空的slab时才会调用页分配函数；
只有在内存变得紧缺时，系统试图释放出更多内存以供使用，或者当高速缓存显示的被撤销时，才会调用释放函数

### 12.6.2 slab分配器的接口
一个新的高速缓存通过以下函数创建
```
struct kmem_cache * kmem_cache_create(const char *name,         // 高速缓存的名字
                                      size_t size,              // 缓存中每个元素的大小
                                      size_t align,             // slab内第一个对象的偏移，来确保页内进行特定的对齐  
                                      unsigned long flags,      // 控制高速缓存的行为，0表示没有特殊行为
                                      void (*ctor)(void *));
```

从缓存中分配：
``void * kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);``
从高速缓存cachep中返回一个指向对象的指针。如果高速缓存所有的slab中都没有空闲的对象，那么slab层必须通过kmem_getpages()获取新的页
释放对象：
``void kmem_cache_free(struct kmem_cache *cachep, void *objp);``
把cachep中的对象objp标记为空闲
slab层负责内存紧缺情况下所有底层的对齐、着色、分配、释放和回收等
如果要频繁创建很多相同类型的对象，那么应该考虑使用slab高速缓存

## 12.7 在栈上的静态分配
内核栈小且固定。给每个进程分配一个固定大小的小栈后，不但可以减少内存的消耗，而且内核也无须负担栈管理任务

## 12.8 高端内存的映射
通过alloc_pages()，以__GFP_HIGHMEM标志获得的页不可能有逻辑地址

### 12.8.1 永久映射
要映射一个给定的PAGE结构到内核地址空间，可以使用``void *kmap(struct page *page)``
这个函数在高端内存或低端内存上都可以使用
1. 如果page结构对应的是低端内存中的一页，函数只会单纯的返回该页的虚拟地址
2. 如果是位于高端内存，则会建立一个永久映射，再返回该地址
该函数可以睡眠，因此只能用在进程上下文中
对应解除映射``void kunmap(struct page * page)``

### 12.8.2 临时映射
在不能睡眠不能阻塞的中断处理程序上下文中，内核可以原子的把高端内存中的一个页映射到某个保留的映射中
``void kmap_atomic(struct page *page, enum km_type type)``
对应解除映射``void kunmap_atomic(void *kvaddr, enum km_type type)``,也不会阻塞

## 12.9 每个CPU的分配
```
unsigned long my_percpu[NR_CPUS];

int cpu;
cpu = get_cpu();    // 获取当前处理器，并禁止内核抢占
my_percpu[cpu]++    // 一些操作
put_cpu();          // 激活内核抢占
```

## 12.10 新的每个CPU接口

### 12.10.1 编译时的每个CPU数据
```
get_cpu_var(name)++;        // 增加该CPU上的NAME变量的值
put_cpu_var(name);          // 完成，重新激活内核抢占

per_cpu(name, cpu)++;       // 增加制定处理器上NAME变量的值。但是不会禁止内核抢占，也不会提供任何锁保护
```

### 12.10.2 运行时的每个CPU数据
```
void *alloc_percpu(type);       // 一个宏
void *__alloc_percpu(size_t size, size_t align);
void free_percpu(const void *);
```

## 12.11 使用每个CPU数据的原因
1. 减少了数据锁定。按照每个CPU访问数据的逻辑，就不需要任何锁
2. 大大减少缓存失效

## 12.12 分配函数的选择
1. 需要连续的物理页 - 低级页分配器／kmalloc()
2. 从高端内存分配 - alloc_pages()
3. 不需要物理上连续的页，仅需要虚拟地址上连续的页 - vmalloc()
4. 要创建和撤销很多大的数据结构 - slab高速缓存


# 虚拟文件系统VFS
``ret = write(fd, buf, len);``
用将buf指针指向的长度为len字节的数据写入fd对应的文件的当前位置

## 13.3 Unix文件系统
4中传统抽象概念：文件，目录项，索引节点和安装点

Unix将文件的相关信息和文件本身区分
文件相关信息（元数据），存储在一个单独的数据结构中，即索引节点。
在磁盘上，文件信息按照索引节点的形式存放在单独的块中；控制信息存储在磁盘的超级块中

## 13.4 VFS对象及其数据结构
4个主要的对象类型
1. 超级块对象 - 代表一个具体的已安装文件系统 - 包括内核针对特定文件系统所能调用的方法，如write_inode()和sync_fs()
2. 索引节点对象 - 代表一个具体的问题 - 包括内核针对特定文件所能调用的方法，如create()和link()
3. 目录项对象 - 代表一个目录项 - 包括内核针对特定目录所能调用的方法，如d_compare()和d_delete()
4. 文件对象 - 代表由进程打开的文件 - 包括进程针对一打开文件的所能调用的方法，如read()和write()

## 13.5 超级块对象
各种文件系统都必须实现，用于存储特定文件系统的信息。对应于存放在磁盘特定扇区中的文件系统超级块或者文件系统控制块
```
struct super_block {
    struct list_heade           s_list;                         // 指向所有超级块的链表
    dev_t                       s_dev;                          // 设备标识符
    unsigned long               s_blocksize;                    // 以字节为单位的块大小
    
    unsigned char               s_blocksize_bits;               // 以位为单位的块大小
    unsigned char               s_dirt;                         // 修改标志
    unsigned long long          s_maxbytes;                     // 文件大小上限
    struct file_system_type     s_type;                         // 文件系统类型
    struct super_operations     s_op;                           // 超级块方法
    strcut dquot_operations     *dp_op;                         // 磁盘限额方法    
    struct quotactl_ops         *s_qcop;                        // 限额控制方法
    struct export_operations    *s_export_op;                   // 导出方法
    unsigned long               s_flags;                        // 挂载标志
    unsigned long               s_magic;                        // 文件系统的幻数    
    struct dentry               *s_root;                        // 目录挂载点
    struct rw_semaphore         s_umount;                       // 卸载信号量
    struct semaphore            s_lock;                         // 超级块信号量
    int                         s_count;                        // 超级块引用计数
    int                         s_need_sync;                    // 尚未同步标志
    atomic_t                    s_active;                       // 活动引用计数
    void                        *s_security;                    // 安全模块
    struct xattr_handler        **s_xattr;                      // 扩展的属性操作
    struct list_head            s_inodes;                       // inodes链表
    struct list_head            s_dirty;                        // 脏数据链表
    struct list_head            s_io;                           // 回写链表
    struct list_head            s_more_io;                      // 更多的回写链表
    struct hlist_head           s_anon;                         // 匿名目录项
    struct list_head            s_files;                        // 被分配文件链表
    struct list_head            s_dentry_lru;                   // 未被使用目录项链表
    int                         s_nr_dentry_unused;             // 链表中目录项的数目
    struct block_device         *s_bdev;                        // 相关块设备
    struct mtd_info             *s_mtd;                         // 存储磁盘信息
    struct list_head            s_instance;                     // 该类型文件系统
    struct quota_info           s_dquot;                        // 限额相关选项
    int                         s_frozen;                       // frozen标志位
    wait_queue_head_t           s_wait_unfrozen;                // 冻结的等待队列
    char                        s_id[32];                       // 文本名字
    void                        *s_fs_info;                     // 文件系统特殊信息
    fmode_t                     s_mode;                         // 安装权限
    struct semaphore            s_vfs_rename_sem;               // 重命名信号量
    u32                         s_time_gran;                    // 时间戳粒度
    char                        *s_subtype;                     // 子类型名称
    char                        *s_options;                     // 已存安装选项
```

## 13.6 超级块操作
s_op指向超级块的操作函数表，由super_operations结构体表示，每一项都是指向一个超级块操作函数的指针。

## 13.7 索引节点对象
索引节点对象包含了内核在操作文件或目录时需要的全部信息

## 13.9 目录项对象
VFS把目录当作文件对待，但解析一个路径并遍历其分量比较复杂。因此，VFS引入了目录项对象
VFS在执行目录操作时，会现场创建目录项对象

### 13.9.1 目录项状态
1. 被使用：对应一个有效的索引节点，并且表明该对象存在一个或者多个使用者。
2. 未被使用：对应一个有效的索引节点，但是当前VFS并未使用它。该目录项对象仍然执行一个有效对象，被保留在缓存中以便需要时再次使用。
3. 负状态：没有对应的有效索引节点。但是目录项仍然保留，以便快速解析以后的路径查询

### 13.9.2 目录项缓存
内核将目录项对象缓存在目录项缓存（dcache)，包括
+ 被使用的目录项链表
+ 最近被使用的双向链表：还有未被使用的和负状态的目录项对象
+ 散列表和响应的散列函数来快速的将给定路径解析为相关目录项对象
dcache也在一定意义上提供对索引节点的缓存，也就是icache。和目录项对象相关的索引节点对象不会被释放

 ## 13.11 文件对象
文件对象表示进程已经打开的文件。由相应的open()系统调用创建，由close()系统调用撤销

## 13.13 和文件系统相关的数据结构
+ file_system_type: 用来描述各种特定文件系统类型，比如ext3, ext4或UDF
+ vfsmount:描述一个安装文件系统的实力

## 13.14 和进程相关的数据结构
每个进程都有自己的一组打开的文件，有3个数据结构将VFS和系统进程联系一起：
+ file_struct
+ fs_struct
+ namespace

struct file_struct {
    atomic_t            count;        // 结构的使用计数
    struct fdtable      *fdt;         // 指向其他fd表的指针
    struct fdtable      fdtab;        // 基fd表
    spinlock_t          file_lock;    // 单个文件的锁
    int                 next_fd;      // 缓存下一个可用的fd
    struct embedded_fd_set close_on_exec_init;    // exec()时关闭的文件描述符链表
    struct embedded_fd_set open_fds_init;         // 打开的文件描述符链表
    struct file         *fd_array[NR_OPEN_DEFAULT];    // 缺省的文件对象数组 
}

如果一个进程所打开的文件对象超过64个，内核将分配一个新数组，并将fdt指向它。所以对适当数量的文件对象的访问会执行的很快，因为它是对静态数组的操作。如果系统中有大量进程打开超过64个文件，为了优化新能，可以适当增大NR_OPEN_DEFAULT的预定义值

struct fs_struct {
    int             users;        // 用户数目
    rwlock_t        lock;         // 保护该结构体的锁
    int             umask;        // 掩码
    int             in_exec;      // 当前正在执行的文件
    struct path     root;         // 根目录路径
    struct path     pwd;          // 当前工作目录的路径
}

struct mmt_namespace {
    atomic_t            count;        // 结构的使用计数
    struct vfsmount     *root;        // 根目录的安装点对象
    struct list_head    list;         // 安装点链表
    wait_queue_head_t   poll;         // 轮询的等待队列
    int                 event;        // 事件计数
}

这3个数据结构都是通过进程描述符链接起来的。对多数进程来说，他们的描述符都指向唯一的file_struct和fs_struct结构体。但是对于使用克隆标志CLONE_FILES和CLONE_FS创建的进程，会共享这2个结构体。

默认情况下，所有进程共享同样的命名空间。只有在clone()操作时使用CLONE_NEWS标志，才会给进程一个唯一的命名空间结构体的拷贝。


# 块I/O层
系统中能够随机访问固定大小数据片的硬件设备称作块设备

## 14.1 剖析一个块设备
块设备中最小的可寻址单位是扇区，常见的为512字节
块是文件系统的一种抽象-只能基于块来访问文件系统，虽然物理磁盘的寻址是按照扇区级进行的，但是在内核中执行的所有磁盘操作都是按照块进行的。
由于扇区是设备的最小可寻址单元，所以块必须比扇区大。一般是若干倍。通常是512B，2KB，4KB

当一个块被调入内存时，它会存在一个缓冲区里。每个缓冲区与一个块对应。每个缓冲区都有一个对应的描述符，存储一些相关的控制信息，称为缓冲区头

struct buffer_head {
    unsigned long b_state;                    // 缓冲区状态标志
    struct buffer_head *b_this_page;          // 页面中的缓冲区
    struct page *b_page;                      // 存储缓冲区的页面
    sector_t b_blocknr;                       // 起始块号
    size_t b_size;                            // 映像大小
    char *b_data;                             // 页面内的数据指针
    struct block_device *b_bdev;              // 相关联的块设备
    bh_end_io_t *b_end_io;                    // IO完成方法
    void *b_private;
    struct list_head b_assoc_buffers;         // 相关的映射链表
    struct address_space *b_assoc_map;        // 相关的地址空间
    atomic_t b_count;                         // 缓冲区的使用计数
}

缓冲区头的目的在于描述磁盘块和物理内存缓冲区的映射关系

## 14.3 bio结构体
目前内核块中块IO操作的基本容器由bio结构体表示。代表了正在现场的以及片断链表形式组织的块IO操作

struct bio {
    sector_t                    bi_sector;            // 磁盘上相关的扇区
    struct bio                  *bi_next;             // 请求链表
    struct block_device         *bi_bdev;             // 相关的块设备
    unsigned long               bi_flags;             // 状态和命令标志
    unsigned long               bi_rw;                // 读or写
    unsigned short              bi_vcnt;              // bio_vecs偏移的个数
    unsigned short              bi_idx;               // bio_io_vecs当前的索引
    unsigned short              bi_phys_segments;     // 结合后的片断数目
    unsigned int                bi_size;              // IO计数
    unsigned int                bi_seg_front_size;    // 第一个可合并的段大小
    unsigned int                bi_seg_back_size;     // 最后一个可合并的段大小
    unsigned int                bi_max_vecs;          // bio_vecs数目上限
    unsigned int                bi_comp_cpu;          // 结束CPU
    atomic_t                    bi_cnt;               // 使用计数
    struct bio_vec              *bi_io_vec;           // bi_vecs链表
    bio_end_io_t                *bi_end_io;           // IO完成方法
    void                        *bi_private;          // 拥有者的私有方法
    bio_destructor_t            *bi_destructor;       // 撤销方法
    struct bio_vec              bi_inline_vecs[0];    // 内嵌bio向量
}

 ### 14.3.1 I/O向量
bi_io_vec指向一个bio_vec结构体数组，包含了一个特定I／O操作所需要使用到的所有片段
struct bio_vec {
    struct page     *bv_page;        // 指向这个缓冲区所驻留的物理页
    unsigned int    bv_len;          // 这个缓冲区以字节为单位的大小
    unsigned int    bv_offset;       // 缓冲区所驻留的页中以字节为单位的偏移量
}

## 14.5 I／O调度程序
为了优化寻址操作，内核会在提交前，执行合并与排序的预操作。

### 14.5.1 I/O调度程序的工作
管理块设备的请求队列。IO调度程序有2种方法减少磁盘寻址时间：合并（合并访问相邻磁盘扇区的请求）与排序（电梯调度算法）

### 14.5.2 Linux电梯
可以处理合并与排序预处理。可以执行向前和向后合并。
如果合并失败，就需要寻找可能的插入点（新请求在队列中的位置必须符合请求以扇区方向有序排序的原则）。如果找到，则插入到该点；如果没有，就加入到队列尾部。
总的来说，一个请求加入到队列时，有可能发生4中操作：
1. 如果队列中已存在一个对相邻磁盘扇区操作的请求，那么新请求将和这个已经存在的请求合并
2. 如果队列中存在一个驻留时间过长的请求，那么新请求将被插入到队列尾部，防止旧的请求发生饥饿
3. 如果队列中以扇区方向为序存在合适的插入位置，那么新的请求将被插入到该位置
4. 如果不存在合适的位置，就插入到尾部

### 14.5.3 最终期限I／O调度程序
因为读操作具有同步性，读请求的响应时间直接影响系统性能。因此2.6版本引入了最后期限I／O调度程序

最后期限I／O调度程序中，每个请求都有一个超时时间。类似于Linux电梯，也以磁盘物理位置为次序维护请求队列。当一个新请求递交给排序队列时，最后期限IO调度程序执行合并和插入请求，但是同时也会以请求类型为依据将它们插入到额外的队列中：读请求-读FIFO队列，写请求-写FIFO队列

### 14.5.5 完全公正的排队I／O调度程序
每一个提交I／O的进程都有自己的队列，以时间片轮转调度队列，从每个队列中选取请求数（默认为4，可配），然后进行下一轮调度。提供了进程级的公平，确保每个进程接收公平的磁盘带宽片段

### 14.5.6 空操作的I/O调度程序
专为随机访问设备而设计。除了执行合并，其他什么都不做


# 进程地址空间

## 15.1 地址空间
进程地址空间由可寻址的虚拟内存组成。每个进程都有一个32／64位的flat（即独立的连续区间）地址空间
尽管一个进程可以寻址4GB的虚拟内存（32位），但是并不代表有权限访问所有的虚拟地址。进程只能访问有效内存区内的内存地址。

内存区域可以包含各种内存对象：
+ 可执行文件代码的内存映射，称为**代码段**
+ 可执行文件的已初始化全局变量的内存映射，称为**数据段**
+ 包含未初始化全局变量，也就算bss段的零页
+ 用于进程用户空间栈的零页的内存映射
+ 每一个诸如C库或动态链接程序等共享库的代码段、数据段和bss也会被载入进程的地址空间
+ 任何内存映射文件
+ 任何共享内存段
+ 任何匿名的内存映射，比如malloc()分配的内存

## 15.2 内存描述苻
mm_struct表示进程的地址空间，包含了和进程地址空间有关的全部信息

所有的mm_struct结构体都通过自身的mmlist域链接在一个双向链表中，该链表的首元素是init_mm内存描述苻，它代表init进程的地址空间。

### 15.2.1 分配内存描述苻
在进程描述苻中,mm字段存放着进程使用的内存描述苻，所以current->mm即当前进程的内存描述苻。
fork()利用copy_mm()复制父进程的内存描述苻给其子进程，而子进程中的mm_struct结构体实际是通过allocate_mm()宏从mm_cachep_slab缓存中分配得到的。通常，每个进程都有**唯一**的mm_struct结构体，即唯一的进程地址空间

如果在创建线程时，即在clone()时设置CLONE_VM标志，内核就不再需要调用allocate_mm()，而仅仅在调用copy_mm()时将mm域指向父进程的内存描述苻即可。

### 15.2.2 撤销内存描述苻
进程退出时，内核会调用exit_mm(),执行一些常规的撤销工作。同时，调用mmput()减少内存描述苻中的mm_user计数，如果mm_user变为0，将调用mm_drop()，减少mm_count计数，如果mm_count变为0，调用free_mm()将mm_struct归还到mm_cachep_slab中。

### 15.2.3 mm_struct与内核线程
内核线程没有进程地址空间，也就没有内存描述苻。这也是内核线程的真实含义 - 没有用户上下文

## 15.3 虚拟内存区域
由vm_area_struct描述，内存区域在linux内核中也经常称为虚拟内存区域(VMAs)
描述了指定地址空间内连续区间上的一个独立内存范围。

## 15.4 操作内存区域

### 15.4.1 find_vma()
找到一个给定的内存地址属于哪一个内存区域
``struct vm_area_struct * find_vma(struct mm_struct *mm, unsigned long addr);``

### 15.4.2 find_vma_prev()
返回第一个小于addr的VMA

### 15.4.3 find_vma_intersection()
返回第一个和指定地址区间相交的VMA

## 15.5 mmap()和do_mmap():创建地址空间
内核使用do_mmap()创建一个新的线形地址空间。会将一个地址区间加入到进程的地址空间中。

##15.6 mummap()和do_mummap()：删除地址区间
从特定的进程地址空间中删除指定地址区间
``int do_mummap(struct mm_struct *mm, unsigned long start, size_t len);``

## 15.7 页表
用于将虚拟地址转换为物理地址。概括的讲，地址转换需要将虚拟地址分段，使每段虚拟地址都作为一个索引指向页表。而页表项指向下一级别的页表或者指向最终的物理页面。

Linux使用三级页表完成地址转换。
顶级页表上页全局目录（PGD），包含了一个pgd_t类型数组。PGD中的表项指向二级目录的表项：PMD
二级页表上中间页目录（PMD），是个pmd_t类型数组，其中表项指向PTE中的表项
最后一级简称页表，包含了pte_e类型的页表项，该页表项指向物理页面

每个进程都有自己的页表，内存描述苻的pgd域指向的就是进程的页全局目录。操作和检索页表时必须使用page_table_lock锁，以防止竞争条件


# 页高速缓存和页回写

## 16.1 缓存手段
页高速缓存由内存中的物理页面组成，其内容对应磁盘上的物理块。

### 16.1.1 写缓存
+ 不缓存：不去缓存任何写操作。直接跳过缓存，写到磁盘。很少使用
+ 写透缓存：写操作会立刻穿透缓存存到磁盘。对保持缓存一致性有好处。
+ 回写：写操作直接写到缓存，后端存储不会立即更新么事将页高速缓存中被写入的页面标记成“脏”，加入到脏页链表中。然后一个进程周期性的将脏页链表中的页写回到磁盘，从而让磁盘中的数据和内存中最终一致

### 16.1.2 缓存回收
Linux缓存回收是通过选择干净页进行简单替换，如果缓存中没有足够的干净页，内核将强制地进行回写操作，以腾出更多干净可用页。
+ LRU 最近最少使用
+ 双链策略：活跃链表和非活跃链表，在活跃链表中的页面必须在其被访问时就处于非活跃链表中。

## 16.2Linux页高速缓存

### 16.2.1 address_space对象




