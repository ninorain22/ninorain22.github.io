#### 运行示例和Shell

在最顶层的Spark目录中使用
``./bin/run-example <class> [params]``

也可以通过Scala shell来交互使用Spark
```sbtshell
# --master 指定针对分布集群的master url
# 或者以local[N]模式使用N个线程在本地运行
./bin/spark-shell --master local[2]
```

### 快速入门
在Spark2.0之前，Spark的主要编程接口依然是RDD，但是在Spark2.0之后，DataSet取代了RDD，RDD接口仍然支持，但是DataSet的表现更为优异。
可以在SQL编程知道中获取DataSet的更多信息

#### 安全
默认情况下，Spark的安全配置是关闭的，参考`Spark Security`进行设置

#### 基础
Spark shell是一种交互式的使用spark工具
```sbtshell
./bin/spark-shell
```

Spark的主要抽象是一个称为`DataSet`的分布式的Item集合。例如，从README文件来创建一个新的DataSet
```scala
val textFile = spark.read.textFile("README.md")
```

可以从DataSet中获取一些values(值)，通过调用一些actions(动作), 或者transform(转换)以获得一个新的DataSet
```scala
textFile.count()
textFile.first()

// transform & action
// 转换成一个新的Dataset
val linesWithSpark = textFile.filter(_.contains("Spark"))
val linesCountWithSpark = linesWithSpark.count
```

#### DataSet上的更多操作
actions和transformations可以用于更复杂的计算，例如统计单词最多的行
```scala
textFile.map(line => line.split(" ").size).reduce((a, b) => if a > b a else b)
```

Spark可以很容易的实现MapReduce
```scala
val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count
```

`groupByKey`和`count`来计算每个单词的counts作为一个(String, Long)的DataSet pairs

要在shell中收集wordCounts, 可以调用`collect`
```scala
wordCounts.collect
```

#### 缓存
Spark支持拉取数据集到一个集群范围的内存缓存中，这在数据被重复访问时是非常高效的

比如查询一个小的热数据集或者执行PageRank等高迭代算法。
```scala
linesWithSpark.cache()
```

### RDD编程指南
在高层面来看，每个Spark应用，由1个在集群上运行`main`函数和执行`parallel`(并行)操作的的Driver Program构成。

一个RDD可以由Hadoop文件系统(或者其他Hadoop支持的文件系统)直接创建而来，或者由一个已经存在在Driver Program中的Scala collection转换而来。

用户也可以`persist`一个RDD在内存中，允许其高效的在跨并行计算中被复用

Spark两大抽象:
+ RDD(弹性分布式数据集)
+ 并行处理中的共享变量，包括广播变量和累加器

#### 初始化Spark
一个Spark程序的第一件事情就是创建一个`SparkContext`对象，告诉Spark如何连接到一个集群，
为了创建一个`SparkContxt`，需要先创建一个`SparkConf`来包含应用的一些配置信息
```scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

// appName用来设置在cluster UI上可见的应用名称
// master可以是一个spark地址(standalone模式)， Mesos(Spark官方推荐), YARN集群URL
// 也可以是"local"来部署本地模式
val conf = new SparkConf().setAppName(appName).setMaster(master)
val sc = new SparkContext(conf)
```

#### 弹性分布式数据集RDD
两种方式创建一个RDD：
+ 在一个已存在的集合(比如数组)上调用`parallelizing`方法
+ 通过外部数据源，比如共享文件系统，HDFS，HBase以及任何提供Hadoop输入格式的数据源

###### parallel collection
parallel集合的一个重要参数是`partition`的数量。
在集群中，Spark会为每个partition启动一个task。一般而言，每个CPU处理2-4个partition
Spark会自动设置这个参数，也可以手动设置
```spark-shell
val data = Array(1, 2, 3)
val distData = sc.parallelize(data, 10)
```
一单被创建，`distData`就可以用于并行操作。比如，可以调用`distData.reduce(_+_)`来计算数组中的元素。我们将稍后讨论在RDD中的操作

##### 外部数据集
Spark支持从本地文件系统、HDFS、Cassandra、HBase、S3等等创建RDD。

文件RDDs可以通过`SparkContext.textFile()`创建
```scala
// 文件的每一行作为一个记录返回
val disFile = sc.textFile("data.txt")
```
一但被创建，`distFile`就可以用被当做dataset来操作。比如，可以统计所有行的长度之和`distFile.map(_.length).reduce(_+_)

文件RDD中需要注意的几个问题:
+ 如果目录是本地文件系统，那么必须在每个worker节点上都放在同一个目录下
+ 所有Spark基于文件输入的方法中，都支持访问目录，压缩文件以及模糊匹配
+ textFile文件也支持设置可选的第二个参数来控制文件的partition number
默认情况下，Spark为文件的每个block（HDFS中默认128M）创建一个partition(分区)，可以设置更多个partition
但不能比blocks的数量少
+ `SparkContext.whileTextFile`可以用户来读取一个目录下的文件，返回(filename, content)的键值数据
+ 对于SequencesFile, 使用`SparkContext.sequenceFile[K, V]`方法
+ `RDD.saveAsObjectFile`和`SparkContext.objectFile`支持将RDD简单的序列化并保存成一个java object
+ 如果一个RDD需要大量的迭代计算，可以使用`persist`或`cache`方法将其留在内存中

##### RDD操作
+ transformations：基于现有的RDD创建一个新的dataset
+ actions: 返回给Driver Program一个在数据集上计算出的值

所有的transformation都是惰性的，只会在遇见action的时候开始真正的操作。这种设计方式使得Spark可以更高效的运行。

一般来说，每次RDD的转换在执行action时都将重新计算，然而模拟可以`persist`一个RDD在内存中，使用`persist()`或者`cache()`方法。
这样，Spark会将这些元素存储在集群范围内的缓存上，下一次查询将变得非常快速。当然，也可以支持将RDD`persist`在磁盘上，或者跨越多个节点保存副本


```scala
val lines = sc.textFile("data.txt")
val lineLengths = lines.map(s => s.length)
lineLengths.cache // 或者lineLengths.persist
val totalLength = lineLengths.reduce(_+_)
// 只要在reduce时，Spark才会将这些计算break成tasks分配到各个独立的机器上计算，然后每个机器在本地运行map的一部分计算
// 仅仅返回最终结果给Driver Program
```

###### 传递函数至Spark
Spark API重度依赖于在Driver Program中将*函数*传递给集群。有两种推荐的方式:
+ 匿名函数: 适用于简短的代码片段
+ 在单例对象中的static方法
```scala
object MyFunctions {
  def func1(s: String): String = s + "suffix"
}
myRDD.map(MyFunctions.func1)

// 虽然也可以将一个类的实例的方法传递，但是这将导致将这个实例所属类也需要传递到集群
class MyClass {
  def func1(s: String): String = s + "suffix"
}
val myClass = new MyClass()
myRDD.map(myClass.func1)

// 同样的，引用闭包外的字段，将导致引用这个对象
class MyClass1 {
  val field = "Hello"
}
val myClass1 = new MyClass1()
myRDD.map(x => myClass1.field + x)
```

#### 理解闭包
在Spark中，一个重要的事情就是理解当在集群上执行代码是，变量和方法的可见范围(scope)和其生命周期(left cycle)

举例说明
```scala
val counter = 0
val rdd = sc.parallelize(data)

// Wrong!!!
rdd.foreach(x => counter += x)
```

##### 本地模式和集群模式
在执行Jobs中，Spark回想正在处理的RDD操作分割成多个tasks，每个task都将由一个executor来执行。
在执行之前，Spark会计算task的闭包。闭包就是这些在executor计算RDD是可见的变量和函数，这个闭包会序列化并发送到每个executor上。

在发送给每个executor的闭包中的变量都将是一份拷贝，因此，上面代码中的foreach里的counter仅仅是一个引用，并不再是Driver节点上的counter.
尽管Driver节点的内存中有这个counter, 单其对每个executor确是不可见的(可见的仅仅是其当时的拷贝)。因此，最后的结果里，counter依然是0

###### 打印RDD的元素
在集群模式中，为了在driver中展示RDD的元素，先用`collect`方法将RDD从各个executor中收集起来，否则的话
``rdd.foreach(println)``的结果将打印在executor的stdout上，而不是Driver Program中
```scala
rdd.collect().foreach(println)
```

###### 键值对
计算每一行文本在文件中的出现次数
```scala
val lines = sc.textFile("data.txt")
val pairs = lines.map((_, 1))
val counts = pairs.reduceByKey(_+_)
```
也可以用`counts.sortByKey()`来对pair排序，再用`collect()`将元素全部收集到driver中

###### Shuffle操作
在Spark中，shuffle机制会被某些操作触发，shuffle是spark重新分配数据的一种机制，使得数据可以在跨不同partition进行分组。
这通常涉及在executor和机器之间拷贝数据，因此复杂且代价高

背景
以`reduceByKey`为例，来看看在shuffle中到底发生了什么。

`reduceByKey`操作生成一个新的RDD，其中的每个key的值都会被连接成一个元组:<key, result of reduce>.
但是，问题是并不是所有key的值都在同一个partition上，甚至都不在一个机器上，但是它们必须被共同计算来得出最终结果。

在Spark里，特定的操作需要数据不跨分区分布，在计算期间，一个分区上会有一个task执行，为了组织所有数据都能在一个单个的`reduceByKey`的reduce task上运行，
还需要一个`all-to-all`的操作，即从所有分区读取所有的key对应的所有值，然后将这些跨分区的数值带到一起来计算出最终的结果，这个过程就叫做shuffle

可以触发shuffle的操作包括`repartition`和`coalesce`, `groupByKey`和`reduceByKey`, `cogroup`和`join`

###### 性能影响
Shuffle操作涉及到磁盘IO、数据序列化、以及网络IO，因此是极其昂贵的操作。
为了shuffle的数据组织，Spark启用了一系列的任务:
+ 一系列的map task来组织数据： map阶段
+ 一系列的reduce task来聚集数据：reduce阶段

某些特定的shuffle操作会消耗大量的堆内存，因为它要使用内存中的数据结构来组织转换前后的记录。
需要特别说明的是`reduceByKey`和`aggregateByKey`，在map阶段中会创建这些结构，而那些`ByKey`操作是在reduce阶段创建这些数据结构。

当内存满时，Spark会把溢出的数据存到磁盘上，这将导致额外的磁盘IO开销和GC开销

shuffle的行为可以通过调节多个参数来调整。见`Spark Configuration`中的`Shuffle Behavior`

#### RDD持久化
Spark一个重要的能力就是在多个操作间的内存中将数据持久化(或者叫caching缓存)。

当持久化一个RDD时，每个节点的其他分区都可以使用RDD在内存中进行计算，在该数据上的其他action操作将直接使用内存中的数据，这样以后的action操作会更快

RDD可以使用`persist()`或`cache()`进行持久化，当这个action第一次执行时，就会被保存在节点的内存中。
Spark的cache是由容错机制的，意思是就是如果RDD的任何partition(分区)有丢失，都会自动的重新计算

每个持久化的RDD可以选择不同的存储级别进程缓存
+ MEMORY_ONLY: 以反序列化的JAVA对象形式存储在JVM中，如果内存空间不够，不再缓存, 默认级别
+ MEMORY_DISK: 以反序列化的JAVA对象形式存储在JVM中，如果内存空间不够，缓存到磁盘
+ MEMORY_ONLY_SER: 以序列化的JAVA对象形式存储，比反序列化节省很多空间，但在读取时会增加CPU计算负担
+ MEMORY_AND_DISK_SER: 类似于MEMORY_ONLY_SER，但是溢出的分区会存储到磁盘
+ DISK_ONLY: 只在磁盘上缓存
+ MEMORY_ONLY_2: 与上面的一样，只不过每个分区在集群中的两个节点上建立副本
+ OFF_HEAP

在shuffle操作中（比如reduceByKey)，即使用户没有调用`persist`方法，Spark也会自动缓存部分中间数据。这样当shuffle过程某个节点失败时，不需要重新计算所有数据

##### 如何选择存储级别
MEMORY_ONLY最高效 => MEMORY_ONLY_SER更加节省空间 => 不要溢出到磁盘，除非计算数据集的函数是昂贵的 => 如果需要快速故障恢复，使用复制的存储级别

##### 删除数据
Spark会自动监听每个节点上的缓存使用情况，使用LRU来丢弃旧数据分区。但也可以手动调用`RDD.upersist()`来删除


#### 共享变量
通常情况，一个传递个Spark操作(比如map, reduce)的函数func是在远程的集群节点上执行的，该函数func在多个节点上执行时使用的变量，是同一个变量的多个副本。
为了实现通用的读写共享变量，spark提供了两种共享变量：广播变量和累加器

##### 广播变量
允许将一个只读的变量缓存到每台机器，而不是传递给任务一个副本，因此可以用来高效的给每个节点共享一份较大的输入DataSet
```scala
val bVar = sc.broadcast(Array[1,2,3])
bVar.value
```

##### 累加器
一个仅仅可执行`add`的变量，可以高效的并行执行
```scala
val accu = sc.longAccumulator("My Accu")
// 在每个task中，可以使用累加器的add方法，但是不能访问它
sc.parallelize(Array(1,2,3,4)).foreach(x => accu.add(x))

// 只有在Driver Program中才可以使用value方法访问累加器
accu.value // 10
```
累加器的更新只发生在action操作中，Spark保证每个任务只更新累加器1次。


#### 部署应用到集群中
将应用打包成jar，然后通过`bin/spark-submit`提交到它任何所支持的cluster manager上

##### 单元测试
将master Url设置为`local`， 然后调用`SparkContext.stop()`停止

